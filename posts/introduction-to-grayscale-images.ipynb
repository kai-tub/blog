{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0df0ea8c",
   "metadata": {},
   "source": [
    "# Introduction to grayscale images\n",
    "> A short introduction to grayscale images and how they are encoded on the computer. \n",
    "\n",
    "- date: \"2020-09-02\"\n",
    "- comments: true\n",
    "- tags: remote-sensing,beginner,grayscale,images\n",
    "- category: remote-sensing-introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ebd90",
   "metadata": {},
   "source": [
    "Before we can dive into multi-spectral satellite images, I think a quick refresher on how images\n",
    "are encoded and represented in memory is a good starting point.\n",
    "\n",
    "## Binary encoding\n",
    "\n",
    "Let's take a short recap of how classical computer vision images are encoded in memory.\n",
    "Internally a computer (ignoring quantum-computing) only works with binary numbers. A binary number is either a 0 or a 1, on or off.\n",
    "The value of such a binary number is called a *bit*.\n",
    "The smallest data-element is called a *byte*. A byte consists of 8 bits.\n",
    "There are different ways how we could use these 8 bits/1 byte to encode our data.\n",
    "The data we are trying to store/load defines how we interpret the data. \n",
    "If we want to only work with positive integers, we use an unsigned integer type.\n",
    "An unsigned integer with 8 bits can encode all numbers from 0$-$255.\n",
    "If all bits are 1, also called *set*, the value is 255.\n",
    "If all bits are 0 the corresponding value is 0.{% fn 1 %}\n",
    "\n",
    "## Grayscale images\n",
    "Images, like everything in a computer, are also only encoded in binary values.\n",
    "The most straightforward images are grayscale images. The possible colors of each pixel of a grayscale image can only range from black to gray to white, with all different gray shades in-between. \n",
    "*Pixels* are the basic elements of a picture. The word itself, [pixel](https://en.wikipedia.org/wiki/Pixel#Etymology), is a combination of the words picture and element/cell. So an image consists of pixels, similar to how a brick wall consists of bricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bdf48",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <div style=\"display: flex; flex-wrap: wrap; justify-content: center\">\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"/images/2020-09-02/brick.jpg\" alt=\"Brick\">\n",
    "            <figcaption><center>Pixel</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "        <div>\n",
    "            <figure>\n",
    "<img src=\"/images/2020-09-02/brick-wall.jpg\" alt=\"Brick Wall\">\n",
    "            <figcaption><center>Complete Image</center></figcaption>\n",
    "            </figure>\n",
    "        </div>\n",
    "    </div>\n",
    "    <figcaption><center>My weird analogy</center></figcaption>\n",
    "</figure>\n",
    "\n",
    "We can understand how simple 8-bit grayscale images are encoded with the knowledge of our previous simple encoding scheme.\n",
    "The prefix 8-bit (usually) refers to the [*color-depth*](https://en.wikipedia.org/wiki/Color_depth). It indicates how many bits are used per channel.\n",
    "We only have a single channel for a grayscale image, the\n",
    "colors range from black to white. (We will take a closer look at different channels in the next post.)\n",
    "For now, we note that our grayscale channel is encoded with 8-bits. Or, put differently, we use 8-bits for every pixel to show different shades of gray. With 8-bits, we can color each pixel in 256 (2‚Å∏) different ways.\n",
    "\n",
    "With the [numpy](https://numpy.org/) and [PIL](https://pillow.readthedocs.io/) library,\n",
    "we can easily create our own 8-bit grayscale image by merely changing the value of a byte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f048bc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAADiCAAAAAB2e0QLAAAASElEQVR4nO3BAQ0AAADCoPdPbQ8HFAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC8GshmAAHtfXLjAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=226x226 at 0x7F9B205D7F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAADiCAAAAAB2e0QLAAABFUlEQVR4nO3PAQ2AMBDAwB/KkD5pqFgWmjsF7Uzemvd2wmH7uV1wnsUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYJFf+AAPvALAPLyc0gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=226x226 at 0x7F9B2065C910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAADiCAAAAAB2e0QLAAABFUlEQVR4nO3PAQ2AMBDAwB/KkD5pqFgWmjsF7Uzemn074bD3uV1wnsUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYJFf+AB/vALAad14GgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=226x226 at 0x7F9B2065CA00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOIAAADiCAAAAAB2e0QLAAABE0lEQVR4nO3PAQ0AIRDAsAP/nh8VhPzSKthm8tZ8rxMuW/t1wX0WCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoHFAosFFgssFlgssFhgscBigcUCiwUWCywWWCywWGCxwGKBxQKLBRYLLBZYLLBYYLHAYoFFfuEAttwCwFDDi3YAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=226x226 at 0x7F9B2065C880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse_hide\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "def to_grayscale_image(x):\n",
    "    grayscale_8_bit_mode = \"L\"\n",
    "    return Image.fromarray(x, mode=grayscale_8_bit_mode)\n",
    "\n",
    "def upscale_image(img, img_width=224, img_height=224):\n",
    "    return img.resize((img_width, img_height), resample=Image.NEAREST)\n",
    "\n",
    "# PIL requires np arrays as input\n",
    "# Datatype is uint8, our unsigned int consisting of 8-bits\n",
    "# zero is our single byte/value with value 0\n",
    "# -> Array has a width and height of 1\n",
    "zero = np.zeros((1, 1), dtype=np.uint8)\n",
    "\n",
    "img_values = {\n",
    "    \"pixel_0\": zero, \n",
    "    \"pixel_64\": zero + 64, \n",
    "    \"pixel_192\": zero + 192, \n",
    "    \"pixel_255\": zero + 255\n",
    "}\n",
    "\n",
    "for name, value in img_values.items():\n",
    "    img = to_grayscale_image(value)\n",
    "    img = upscale_image(img)\n",
    "    bordered_img = ImageOps.expand(img, border=1, fill=\"black\")\n",
    "    display(bordered_img) # To display in jupyter\n",
    "    # bordered_img.save(f\"2020-09-02/{name}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e938f7",
   "metadata": {},
   "source": [
    "Until now, we did not care about the [resolution](https://en.wikipedia.org/wiki/Image_resolution#Pixel_resolution) of our images. \n",
    "The resolution defines how many pixels we use to visualize the object. A resolution of 1 corresponds to a single pixel.\n",
    "But, with a single-pixel picture, we cannot retain a lot of information. As shown above, we could only create a single shade of gray.\n",
    "Let's increase our resolution for the following images to a size of 224 pixels x 224 pixels. With more pixels, we can show more levels of detail.\n",
    "\n",
    "Now we can extend our previous code to draw gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff4aa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAAA+0lEQVR4nO3PAREAMAgAIV3/0MvxHjRgZ297c5xgnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYJ1gnWCdYN0HObkCn0VNsNgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224 at 0x7F9AF8486F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAABDUlEQVR4nO3PAQkAIBDAQB/sn9kUIpO7BNtan5t5XXDZNhhnsM5gncE6g3UG6wzWGawzWGewzmCdwTqDdQbrDNYZrDNYZ7DOYJ3BOoN1BusM1hmsM1hnsM5gncE6g3UG6wzWGawzWGewzmCdwTqDdQbrDNYZrDNYZ7DOYJ3BOoN1BusM1hmsM1hnsM5gncE6g3UG6wzWGawzWGewzmCdwTqDdQbrDNYZrDNYZ7DOYJ3BOoN1BusM1hmsM1hnsM5gncE6g3UG6wzWGawzWGewzmCdwTqDdQbrDNYZrDNYZ7DOYJ3BOoN1BusM1hmsM1hnsM5gncE6g3UG6wzWGawzWGewzmCdwTqDdQbrDNYdTtkEWbGESgcAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224 at 0x7F9AF8486F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAABDElEQVR4nO3PAQ0AIAzAMI5/0aggpGRVsM2av+3XAbc1qGtQ16CuQV2DugZ1Deoa1DWoa1DXoK5BXYO6BnUN6hrUNahrUNegrkFdg7oGdQ3qGtQ1qGtQ16CuQV2DugZ1Deoa1DWoa1DXoK5BXYO6BnUN6hrUNahrUNegrkFdg7oGdQ3qGtQ1qGtQ16CuQV2DugZ1Deoa1DWoa1DXoK5BXYO6BnUN6hrUNahrUNegrkFdg7oGdQ3qGtQ1qGtQ16CuQV2DugZ1Deoa1DWoa1DXoK5BXYO6BnUN6hrUNahrUNegrkFdg7oGdQ3qGtQ1qGtQ16CuQV2DugZ1Deoa1DWoa1DXoK5BXYO6BnUN6g5408W/Nyv+8wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224 at 0x7F9AF8486F10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAADgCAAAAAA/RjU9AAADZ0lEQVR4nN3P15HDMBQDQGpG/bfM+/DZlmQFhhcAbAe7lEXbWpciba3iw7UW7eFai/ZwrUV7uNZSpIevoPDwP6g7fAdlh5+g6vAbFB1ugprDbVByuAsqDvdBweEhqDc8BuWGP0G14W9QbHgS1BqeBaWGp0Gl4XlQaHgR1BleBWWGl0GV4XVQZHgT1BjeBSWGt0GF4X1QYPgQ5B8+BemHj0H24XOQfNgQ5B62BKmHTUHmYVuQeNgY5B22BmmHzUHWYXuQdNgR5Bz2BCmHXUHGYV+QcNgZ5Bv2BumG3UG2YX+QbDgQ5BqOBKmGQ0Gm4ViQaDgY5BmOBmmGw0GW4XiQZDgR5BjOBCmGU0GG4VyQYDgZxB/OBuGH00H04XwQfGgQxB5aBKGHJkHkoU0QeGgUxB1aBWGHZkHUoV0QdGgYxBxaBiGHpkHEoW0QcGgcxBtaB+GG5kG0oX0QbOgQxBp6BKGGLkGkoU8QaOgUxBl6BWGGbkGUoV8QZOgYxBh6BiGGrkGEoW8QYOgczB96B9OH7sHsoX8weRgQzB1GBFOHIcHMYUwwcRgUzBtGBdOGYcGsYVwwaRgYzBlGBlOGocGMYWwwYRgcjB9GB8OH4cHoYXwweJgQjB1mBEOHKcHIYU4wcJgUjBtmBcOGacGoYV4waJgYjBlmBkOGqcGIYW4wYJgc9B9mB92H6UHvYX7QeQgQ9B0iBF2HEEHPIUbQcQgS9BuiBN2GMEGvIU7QaQgU9BkiBV2GUEGPIVbQYQgWtB+iBc2HcEHrIV7QeAgYtB0iBk2HkEHLIWbQcAgatBuiBs2GsEGrIW7QaAgctBkiB02G0EGLIXbQYAgenB+iB6eH8MHZIX5wckgQnBsyBKeGFMGZIUdwYkgSHB+yBIeHNMHRIU9wcEgUHBsyBYeGVMGRIVdwYEgW7B+yBbuHdMHeIV+wc0gY7BsyBruGlMGeIWewY0gabB+yBpuHtMHWIW+wcUgcbBsyB5uG1MGWIXewYUgefB6yBx+H9MGnIX/wYSgQvB8qBG+HEsG7oUbwZigSvB6qBC+HMsGroU7wYigUPB8qBU+HUsGzoVbwZCgW/B2qBX+GcsHjUC94GAoG90PF4G4oGdwONYOboWjwO1QNfoaywfdQN/g/FA6+hsrBUhfxYKmLeLDU5Q9xHsCOhfVsEgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=224x224 at 0x7F9B10068280>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collapse_input\n",
    "gradients = {}\n",
    "zeros = np.zeros((224, 224), dtype=np.uint8)\n",
    "x_gradient = np.arange(0, 224, dtype=np.uint8).reshape(1, 224)\n",
    "y_gradient = np.arange(0, 224, dtype=np.uint8).reshape(224, 1)\n",
    "\n",
    "# Using numpy's broadcasting\n",
    "gradients[\"x_grad_2d\"] = zeros + x_gradient\n",
    "gradients[\"y_grad_2d\"] = zeros + y_gradient\n",
    "gradients[\"sum_grad_2d\"] = x_gradient + y_gradient\n",
    "gradients[\"diff_grad_2d\"] = x_gradient - y_gradient\n",
    "\n",
    "for data in gradients.values():\n",
    "\tdisplay(to_grayscale_image(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587cf21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "env": {},
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nikola": {
   "category": "",
   "date": "2021-11-08 10:46:03 UTC+01:00",
   "description": "",
   "link": "",
   "slug": "introduction-to-grayscale-images",
   "tags": "",
   "title": "Introduction to grayscale images",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
